{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvNet_4.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vknalluri/64061/blob/main/ConvNet_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tTrain (1000), Validation (500) and Test (500) (Optimizer: RMSprop)\n",
        "\n",
        "a)\tTraining accuracy of the model is highest (99.05%) and the loss is minimum (0.0312) at 26 out of 30 epochs. As such both the metrics are affected after 26 epochs.\n",
        "b)\tValidation accuracy has not shown much improvement after about 15 epochs , topping a maximum of 75%, and the loss actually started to going up after 15 epochs. Apparently, the model at this juncture is overfitting (high training accuracy than that of validation and test) due to , perhaps, due to the small sample.\n",
        "c)\tTest accuracy is 76%.\n",
        "\n",
        "1.\tA) Train (1000), Validation (500) and Test (500) (Optimizer: adam)\n",
        "\n",
        "\n",
        "Training, Validation and test accuracy achieved 100%,75.20% and 76% respectively. Training accuracy topped at 23 epochs.\n",
        "     \n",
        "\n",
        "2.\tTrain (2000), Validation (500) and Test (500) with image augmentation (zoom, rotation and horizontal flip) and Optimizer: adam\n",
        "\n",
        "a)\tTraining accuracy of the model is highest (85.10%)) at 29 out of 30 epochs. \n",
        "b)\tValidation accuracy has topped out at 83.8% at 25 epochs. Validation and training accuracy are almost in tandem.\n",
        "c)\tSurprisingly, Test accuracy is at 85.8%, higher than the training accuracy. This could be due to several factors such as tight regularization, size of test sample or could be its a sheer accident.\n",
        "\n",
        "3.\tTrain (4000), Validation (500) and Test (500) with image augmentation (zoom, rotation and horizontal flip) and Optimizer: adam\n",
        "\n",
        "a)\tTraining accuracy of the model is highest 90.29%)) at 29 out of 30 epochs. \n",
        "b)\tValidation accuracy has topped out at 89.3% at 28 epochs. Validation and training accuracy are almost in tandem.\n",
        "c)\tSurprisingly, Test accuracy is at 90.7%, higher than the training accuracy by a slight margin again. \n",
        "\n",
        "4. All the three scenarios above with Pretrained model. 1(A) is not included:\n",
        "\n",
        "a)  Training accuracy topped out at 98.65% at 11th epoch(either dropped or negligible improvement after 11th epoch) and Validation accuracy is 97.50% at 13th epoch and Test Accuracy at 97.90%\n",
        "b)  Training and validation accuracy are at 96.5 and test accuracy is 97%\n",
        "c)  Training and validation accuracy are at 98.2 and 98 % respectively and test accuracy is 98.1%\n",
        "\n",
        "\n",
        "Note: Changing the strides to 2, in the given below script its none which is equal to a default value of 1, and max pooling size which maintains the spatial invariance & speed & over fit issues may have an impact on the accuracyn of the model. Furthermore, having a bigger training and test sample would effect the accuracy of the model. Of course, changing the filter size also would effect the model performance, for example, making the kernel/filter size from 3X3 to 5X5 may induce more complexity into the model and may either possitively or negatively effect the model performance based on other factors such as size of the data set and other complexities and hyper parameters in the model.\n",
        "\n",
        "Update: 03/28/2022 - All the scenarios above are implemented with a fully connected convoluted network. Though 3rd layer onwards act as fully connected dense network, I just wanted to check the accuracy by replacing the 3rd and 4th convoluted network with a fully connected dense network with 128 nodes.\n",
        "\n",
        "Ran the model on scenario 3 above (4000 training samples etc) and the results were pretty bad , with a training accuracy of about 83% as against over 90% with fully connected convolutional network.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n27_1urVPKvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Data from Kaggle"
      ],
      "metadata": {
        "id": "8DWR2uptj1_e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q0c9oQPfj0up",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "1787938d-e9c7-4ffb-a1d8-bcd4d93fb70b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d5f248bd-7a99-4d8d-94ec-8184fbea6e1f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d5f248bd-7a99-4d8d-94ec-8184fbea6e1f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading dogs-vs-cats.zip to /content\n",
            " 99% 801M/812M [00:05<00:00, 150MB/s]\n",
            "100% 812M/812M [00:05<00:00, 149MB/s]\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c dogs-vs-cats\n",
        "!unzip -qq dogs-vs-cats.zip\n",
        "!unzip -qq train.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partition the Data into (with required number of Images) Train, Validation and Test sets"
      ],
      "metadata": {
        "id": "zqznAspEj3Ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "make_subset(\"test\", start_index=0, end_index=500)\n",
        "make_subset(\"validation\", start_index=500, end_index=1000)\n",
        "make_subset(\"train\", start_index=1000, end_index=5000)"
      ],
      "metadata": {
        "id": "PgvIEJsZkAp5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Pre-Processing"
      ],
      "metadata": {
        "id": "UfcfNUFanzDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n"
      ],
      "metadata": {
        "id": "gUlmDzYynvlU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08fd563-15fc-473f-cf4e-8e656ba0c7e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "random_numbers = np.random.normal(size=(1000, 16))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(random_numbers)\n",
        "\n",
        "for i, element in enumerate(dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "-5QeLbvZnyKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c153a10a-d442-4f58-e9dc-1b22774609cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16,)\n",
            "(16,)\n",
            "(16,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batched_dataset = dataset.batch(32)\n",
        "for i, element in enumerate(batched_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "gNxp_SFcnyuN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b711b42-08bf-4b27-a7f3-621bc724f0f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 16)\n",
            "(32, 16)\n",
            "(32, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4)))\n",
        "for i, element in enumerate(reshaped_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "m8ohfm_koQI6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c548b857-0673-448d-f14a-619f0d2d601c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 4)\n",
            "(4, 4)\n",
            "(4, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying the shapes and labels of the Dataset"
      ],
      "metadata": {
        "id": "sx2MsCR0odaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for data_batch, labels_batch in train_dataset:\n",
        "    print(\"data batch shape:\", data_batch.shape)\n",
        "    print(\"labels batch shape:\", labels_batch.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "oAdwpYFQocEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a02db377-16f4-4510-8540-15e84d1eafe3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data batch shape: (32, 180, 180, 3)\n",
            "labels batch shape: (32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a data augmentation stage to add to an image model"
      ],
      "metadata": {
        "id": "UoOsK3UmK3SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Nh2fWxRXK3rR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying some randomly augmented training images"
      ],
      "metadata": {
        "id": "q3v_0-IUK-U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_dataset.take(1):\n",
        "    for i in range(9):\n",
        "        augmented_images = data_augmentation(images)\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "JUTOm72rK-fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the model (Includes image augmentation and dropout)\n"
      ],
      "metadata": {
        "id": "OTFV9J5hkUiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "#x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "#x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "#x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "#x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "#x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "x= layers.Dense(units=128, activation='relu')(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "-BDMxVYLkT-G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "IX5b6urJlGvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1007f8ca-a308-4a86-f50a-c7ef8b44b6a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, 180, 180, 3)       0         \n",
            "                                                                 \n",
            " rescaling (Rescaling)       (None, 180, 180, 3)       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 178, 178, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 89, 89, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 87, 87, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 43, 43, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 118336)            0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 118336)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               15147136  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,166,657\n",
            "Trainable params: 15,166,657\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuring the model for training"
      ],
      "metadata": {
        "id": "p1NdofS8lcll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "zQ4jIviClcIr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitting the model to the Dataset"
      ],
      "metadata": {
        "id": "yS0AECOoo5jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch.keras_augment_4000\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "AgYk1kh5mMht",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6917c7f2-037b-4487-afad-bafacf459570"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.7069 - accuracy: 0.5760INFO:tensorflow:Assets written to: convnet_from_scratch.keras_augment_4000/assets\n",
            "250/250 [==============================] - 60s 237ms/step - loss: 0.7069 - accuracy: 0.5760 - val_loss: 0.6394 - val_accuracy: 0.6540\n",
            "Epoch 2/30\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.6348 - accuracy: 0.6406INFO:tensorflow:Assets written to: convnet_from_scratch.keras_augment_4000/assets\n",
            "250/250 [==============================] - 59s 238ms/step - loss: 0.6348 - accuracy: 0.6406 - val_loss: 0.5713 - val_accuracy: 0.7110\n",
            "Epoch 3/30\n",
            "250/250 [==============================] - 57s 227ms/step - loss: 0.6053 - accuracy: 0.6744 - val_loss: 0.6259 - val_accuracy: 0.6400\n",
            "Epoch 4/30\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.5746 - accuracy: 0.6914INFO:tensorflow:Assets written to: convnet_from_scratch.keras_augment_4000/assets\n",
            "250/250 [==============================] - 59s 238ms/step - loss: 0.5746 - accuracy: 0.6914 - val_loss: 0.5665 - val_accuracy: 0.7100\n",
            "Epoch 5/30\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.5643 - accuracy: 0.7082INFO:tensorflow:Assets written to: convnet_from_scratch.keras_augment_4000/assets\n",
            "250/250 [==============================] - 59s 236ms/step - loss: 0.5643 - accuracy: 0.7082 - val_loss: 0.5246 - val_accuracy: 0.7540\n",
            "Epoch 6/30\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.5506 - accuracy: 0.7135INFO:tensorflow:Assets written to: convnet_from_scratch.keras_augment_4000/assets\n",
            "250/250 [==============================] - 59s 237ms/step - loss: 0.5506 - accuracy: 0.7135 - val_loss: 0.5135 - val_accuracy: 0.7470\n",
            "Epoch 7/30\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.5298 - accuracy: 0.7300INFO:tensorflow:Assets written to: convnet_from_scratch.keras_augment_4000/assets\n",
            "250/250 [==============================] - 60s 238ms/step - loss: 0.5298 - accuracy: 0.7300 - val_loss: 0.5042 - val_accuracy: 0.7570\n",
            "Epoch 8/30\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 0.5191 - accuracy: 0.7404 - val_loss: 0.5069 - val_accuracy: 0.7580\n",
            "Epoch 9/30\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.5041 - accuracy: 0.7469INFO:tensorflow:Assets written to: convnet_from_scratch.keras_augment_4000/assets\n",
            "250/250 [==============================] - 59s 236ms/step - loss: 0.5041 - accuracy: 0.7469 - val_loss: 0.4923 - val_accuracy: 0.7670\n",
            "Epoch 10/30\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 0.4973 - accuracy: 0.7548 - val_loss: 0.5143 - val_accuracy: 0.7550\n",
            "Epoch 11/30\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.4878 - accuracy: 0.7619INFO:tensorflow:Assets written to: convnet_from_scratch.keras_augment_4000/assets\n",
            "250/250 [==============================] - 61s 243ms/step - loss: 0.4878 - accuracy: 0.7619 - val_loss: 0.4858 - val_accuracy: 0.7710\n",
            "Epoch 12/30\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.4806 - accuracy: 0.7690INFO:tensorflow:Assets written to: convnet_from_scratch.keras_augment_4000/assets\n",
            "250/250 [==============================] - 60s 241ms/step - loss: 0.4806 - accuracy: 0.7690 - val_loss: 0.4620 - val_accuracy: 0.7810\n",
            "Epoch 13/30\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 0.4730 - accuracy: 0.7700 - val_loss: 0.5158 - val_accuracy: 0.7330\n",
            "Epoch 14/30\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.4637 - accuracy: 0.7799INFO:tensorflow:Assets written to: convnet_from_scratch.keras_augment_4000/assets\n",
            "250/250 [==============================] - 60s 241ms/step - loss: 0.4637 - accuracy: 0.7799 - val_loss: 0.4461 - val_accuracy: 0.7920\n",
            "Epoch 15/30\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.4529 - accuracy: 0.7841INFO:tensorflow:Assets written to: convnet_from_scratch.keras_augment_4000/assets\n",
            "250/250 [==============================] - 59s 237ms/step - loss: 0.4529 - accuracy: 0.7841 - val_loss: 0.4448 - val_accuracy: 0.8020\n",
            "Epoch 16/30\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 0.4495 - accuracy: 0.7919 - val_loss: 0.4486 - val_accuracy: 0.7930\n",
            "Epoch 17/30\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 0.4477 - accuracy: 0.7862 - val_loss: 0.4550 - val_accuracy: 0.7950\n",
            "Epoch 18/30\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 0.4352 - accuracy: 0.7944 - val_loss: 0.4686 - val_accuracy: 0.7860\n",
            "Epoch 19/30\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.4329 - accuracy: 0.7934INFO:tensorflow:Assets written to: convnet_from_scratch.keras_augment_4000/assets\n",
            "250/250 [==============================] - 63s 253ms/step - loss: 0.4329 - accuracy: 0.7934 - val_loss: 0.4278 - val_accuracy: 0.8080\n",
            "Epoch 20/30\n",
            "250/250 [==============================] - 61s 244ms/step - loss: 0.4282 - accuracy: 0.8010 - val_loss: 0.4691 - val_accuracy: 0.7760\n",
            "Epoch 21/30\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 0.4178 - accuracy: 0.8106 - val_loss: 0.4360 - val_accuracy: 0.8000\n",
            "Epoch 22/30\n",
            "250/250 [==============================] - 59s 234ms/step - loss: 0.4220 - accuracy: 0.8055 - val_loss: 0.4284 - val_accuracy: 0.8040\n",
            "Epoch 23/30\n",
            "250/250 [==============================] - ETA: 0s - loss: 0.4184 - accuracy: 0.8077INFO:tensorflow:Assets written to: convnet_from_scratch.keras_augment_4000/assets\n",
            "250/250 [==============================] - 60s 240ms/step - loss: 0.4184 - accuracy: 0.8077 - val_loss: 0.4245 - val_accuracy: 0.8100\n",
            "Epoch 24/30\n",
            "250/250 [==============================] - 57s 230ms/step - loss: 0.4041 - accuracy: 0.8155 - val_loss: 0.4370 - val_accuracy: 0.8010\n",
            "Epoch 25/30\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 0.4037 - accuracy: 0.8133 - val_loss: 0.4261 - val_accuracy: 0.8090\n",
            "Epoch 26/30\n",
            "250/250 [==============================] - 58s 234ms/step - loss: 0.4078 - accuracy: 0.8131 - val_loss: 0.4440 - val_accuracy: 0.7980\n",
            "Epoch 27/30\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 0.4008 - accuracy: 0.8150 - val_loss: 0.4455 - val_accuracy: 0.8080\n",
            "Epoch 28/30\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 0.3972 - accuracy: 0.8171 - val_loss: 0.4406 - val_accuracy: 0.7990\n",
            "Epoch 29/30\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 0.3910 - accuracy: 0.8248 - val_loss: 0.4367 - val_accuracy: 0.7940\n",
            "Epoch 30/30\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 0.3844 - accuracy: 0.8261 - val_loss: 0.4571 - val_accuracy: 0.7930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying curves of loss and accuracy during training"
      ],
      "metadata": {
        "id": "PCm6bqUApEZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mzqsYqtopEnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the model on the Test set"
      ],
      "metadata": {
        "id": "Gt_acKpZpOnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = keras.models.load_model(\"convnet_from_scratch.keras_augment_4000\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "g7mibUGTpOzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given below script is to use with pretrained model"
      ],
      "metadata": {
        "id": "1qpVE6bFex__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c dogs-vs-cats\n",
        "!unzip -qq dogs-vs-cats.zip\n",
        "!unzip -qq train.zip"
      ],
      "metadata": {
        "id": "oMZ440eReyTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "make_subset(\"test\", start_index=0, end_index=500)\n",
        "make_subset(\"validation\", start_index=500, end_index=1000)\n",
        "make_subset(\"train\", start_index=1000, end_index=5000)"
      ],
      "metadata": {
        "id": "pGovvikvfLZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n"
      ],
      "metadata": {
        "id": "zhrKuyivfRjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "random_numbers = np.random.normal(size=(1000, 16))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(random_numbers)\n",
        "\n",
        "for i, element in enumerate(dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "C2N_8R6nfXxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batched_dataset = dataset.batch(32)\n",
        "for i, element in enumerate(batched_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "F4rkaFQZfcwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4)))\n",
        "for i, element in enumerate(reshaped_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ],
      "metadata": {
        "id": "_MvXe0MHfj7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data_batch, labels_batch in train_dataset:\n",
        "    print(\"data batch shape:\", data_batch.shape)\n",
        "    print(\"labels batch shape:\", labels_batch.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCxWa0qsf5ZO",
        "outputId": "7f35c6a0-8a09-4f9c-9df3-6ace09e4986e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data batch shape: (32, 180, 180, 3)\n",
            "labels batch shape: (32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extraction together with data augmentation\n",
        "Instantiating and freezing the VGG16 convolutional base"
      ],
      "metadata": {
        "id": "CAtRDkAff_r3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "conv_base  = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False)\n",
        "conv_base.trainable = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xnqv1QTQf6y0",
        "outputId": "ee3f0d52-5585-486e-cb53-e53b8e1c9654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the list of trainable weights before and after freezing"
      ],
      "metadata": {
        "id": "x-CyTy9lgG1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable = True\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"before freezing the conv base:\", len(conv_base.trainable_weights))"
      ],
      "metadata": {
        "id": "K5CakEe3gHLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable = False\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"after freezing the conv base:\", len(conv_base.trainable_weights))"
      ],
      "metadata": {
        "id": "iED3OtJwgHRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding a data augmentation stage and a classifier to the convolutional base"
      ],
      "metadata": {
        "id": "Do2AuTolgWZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = keras.applications.vgg16.preprocess_input(x)\n",
        "x = conv_base(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "orHjp_CJgZ73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "KBAo0ugQgg0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the model on the test set\n",
        "\n"
      ],
      "metadata": {
        "id": "fABh5uTwghFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = keras.models.load_model(\n",
        "    \"feature_extraction_with_data_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kjKr9F1goZ4",
        "outputId": "9b70f658-217e-4874-b5df-a3fe4460d1d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 19s 598ms/step - loss: 1.9534 - accuracy: 0.9790\n",
            "Test accuracy: 0.979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "conv_base  = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False)\n",
        "conv_base.trainable = False"
      ],
      "metadata": {
        "id": "XwcoF9uorWpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the model on the trained model with optimization"
      ],
      "metadata": {
        "id": "Bfr_CM_74Lov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary()"
      ],
      "metadata": {
        "id": "8mFAZDyirBbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.trainable = True\n",
        "for layer in conv_base.layers[:-4]:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "jNwTAgr6rGtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = keras.applications.vgg16.preprocess_input(x)\n",
        "x = conv_base(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "KtJd6obdr0R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"fine_tuning.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=15,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "16zMIHiArH51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"fine_tuning.keras\")\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ],
      "metadata": {
        "id": "BF_pa8bMrLD4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}